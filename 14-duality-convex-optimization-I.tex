\documentclass[main]{subfiles}
\begin{document}

%@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
% Main Topics: Lagrange function, subgradients, projection gradient method
% Convex Optimization Duality I - 06.11.2017
% author: Vanessa Leite

\section{Convex Optimization Duality I}

\subsection{The Lagrange function}

\paragraph{Definition - Lagrange function}
Let $f$ and $g_1, \dots, g_m$ continuous and convex functions. Let the vector
$g(x) =
\begin{pmatrix}
g_1(x)\\
\vdots\\
g_m(x)
\end{pmatrix}
\in \R^m$. Let $D = \bigcap_{i=1}^m dom(g_i)$, $D \subseteq dom(f)$.\\
Consider \\
\begin{equation}
\label{eq:*}
f^* = \min\{f(x): g(x) \leq b\}
\end{equation}
$b \in \R^m$ given. The function
$Z: \R^m_+ \mapsto \R$: $Z(\lambda) = \displaystyle \min_{x \in D} \{f(x) + 
\lambda^T (g(x) -b)\}$ is called the lagrange function for~\ref{eq:*}
(the previous minimization problem).\\

We want to maximize $Z(\lambda)$ to achieve $f^*$.

\paragraph{Lemma:}
\begin{enumerate}
\item $Z(\lambda) \leq f^*$ $\forall \lambda \geq 0$ ($\lambda$ is a vector)
\item $Z: \R^m_+ \mapsto \R$ is concave
\end{enumerate}

\subparagraph{Proof:}
\begin{enumerate}
\item Let $x^*$ be optimal for~\ref{eq:*}, then $g_i(x^*) \leq b_i \forall i$\\
$Z(\lambda) \leq \underbrace{f(x^*)}_{f^*} + \underbrace{\lambda^T}_{\geq 0}
(\underbrace{g(x^*) - b}_{\leq 0}) \leq f^*$
\item Let $\lambda_1 , \lambda_2 \in \R^m$ and $\alpha \in (0,1)$. $\lambda =
\alpha \lambda_1 + (1-\alpha)\lambda_2$\\
Let $\hat{x} \in D$ be such that $Z(\lambda) = f(\hat{x}) + \lambda^T 
(g(\hat{x}) - b)$\\
In particular, $\forall i \in 1,2$: $Z(\lambda_i) \leq f(\hat{x}) + \lambda_i^T 
(g(\hat{x}) - b) \rightarrow \alpha Z(\lambda_1) + (1-\alpha)Z(\lambda_2) \leq 
\alpha (f(\hat{x}) + \lambda_1^T (g(\hat{x}) - b) + (1 -\alpha) (f(\hat{x}) + 
\lambda_2^T (g(\hat{x}) - b)) = f(\hat{x}) + \underbrace{(\alpha \lambda_1 + 
(1-\alpha)\lambda_2)^T}_{\lambda^T} (g(\hat{x}) - b) = Z(\lambda)$.
\end{enumerate}

\paragraph{How to determine $\displaystyle \max_{\lambda \geq 0} Z(\lambda)$?}
Remember: $Z(\lambda)$ is concave!

\paragraph{Definition - subgradient}
Let $f$ be continuously and convex. Given $x \in dom(f)$, a vector $s \in \R^n$
is called a \emph{subgradient} if $f(y) - f(x) \geq s^T(y-x)$ $\forall y \in
dom(f)$, i.e., $\delta f(x) = \{s \in \R^n \mid s \text{ subgradient of } x\}$.

\todo[inline]{add a figure here - check lecture annotations}

\paragraph{Lemma: Let $f$ be convex and continuous. $x^*$ is minimum of $f
\iff \vec{0} \in \partial f(x^*)$.}

\subparagraph{Proof:}
$f(x) \geq f(x^*) \forall x \in dom(f) \iff f(x) - f(x^*) \geq 0 = 0^T (x -
x^*)$.

\paragraph{Remark: existence of subgradients}
Let $f$ be continuous and convex. Then $\forall x_0 \in dom(f)$, $\partial
f(x_0) \neq 0$ and $\forall d \in \R^m$ we have that $$\lim_{\epsilon\to 0}
\frac{f(x_0 + \epsilon d) - f(x_0)}{\epsilon} = \sup \{h^T d \mid h \in
\partial f(x_0)\}$$


\subsection{The projection (sub) gradient algorithm}
Not polytime: pseudo polytime algorithm.\\
Easy version: you can use the gradient, but you can use (plug) any
subgradient.\\

\paragraph{Motivation:} compute $\displaystyle \max_{\lambda \geq 0}
Z(\lambda)$ or directly~\ref{eq:*}. We want to show that both attains the same
results, under some assumptions.

$f* = \min \{f(x) \mid x \in Q \}$ and $x^*$ is a minimizer.

\begin{itemize}
\item $Q$ is convex, non-empty, compact
\item $Q \subseteq dom(f)$
\item $Q$ has diameter $D > 0$, i.e, $|| x-y||_2 \leq D$ $\forall x, y \in Q$
\end{itemize}

\paragraph{Projection oracle:} input $y \in \R^n$, output $PO(y) \in Q$
attaining
$\min \{ \underbrace{||x-y||_2}_{ \text{strictly convex, projection is unique}}
\mid x \in Q \}$.

We know $||x - PO(y)|| \leq ||x-y|| \forall x \in Q$.\\

$f$ is convex, Lipschitz-continuous and differentiable on $dom(f)$:\\

$\vec{0} \leq |f(x) -f(y)| \leq L ||x-y||$ $ \forall x, y \in dom(f)
\rightarrow ||\nabla f(x)|| \leq L$ $\forall x \in dom(f)$.\\

$\vec{0} =
\abs{ \nabla f(x)^T (y-x) }
\rightarrow y = \lambda \nabla f(x) + r$,
$r \perp \nabla f(x)$.

\subparagraph{Proof:}
\todo[inline]{proof as homework}
tips: $f(x) - f(x_t) \geq \nabla f(x_t)^T (x - x_t)$. play with: $x = \lambda
\nabla f(x_t) + r^{\perp \nabla f(x)}$.

\paragraph{Algorithm:}

\begin{enumerate}
\item Choose $\epsilon > 0$, $x_0 \in Q$. Set $\lambda = \frac{2\epsilon}{5L}$,
$T = \lceil (\frac{5LD}{4\epsilon})^2 \rceil$. (Since it depends on the
diameter, it is not polytime.)
\item For $t = 0, \dots, T-1$:
\subitem compute $y_{t+1} = x_t - \underbrace{\frac{\lambda}{||\nabla
f(x_t)||}}_{\text{scale, walks in the negative direction}} \nabla f(x_t)$,
$x_{t+1} = PO(y_{t+1})$.
\item return best point $\bar{x} = \min \{f(x_t) \forall t\}$ that was
generated.
\end{enumerate}

\paragraph{Theorem: $f(\bar{x}) - f^* \leq \epsilon$}
\subparagraph{Proof:}
$\norm{x_{t+1} - x^*}^2 = \norm{PO(y_{t+1}) - x^*}^2 \leq
\norm{y_{t+1} - x^* }^2 = \norm{ x_t - \frac{\lambda}{\norm{\nabla f(x_t)}}
\nabla f(x_t) - x^*}^2$
$= \norm{x_t - x^*}^2 - 2\frac{\lambda \nabla f(x_t)^T} {\norm{\nabla f(x_t)}}
(x_t - x^*) + \lambda^2 = \norm{x_t - x^*}^2 + \frac{2\lambda \nabla f(x_t)}
{\norm{ \nabla f(x_t)}} (x^* - x_t) + \lambda^2$
$\leq \norm{x_t - x^*} + \frac{2 \lambda}{\norm{\nabla f(x_t)}} (f(x^*) -
f(x_t)) + \lambda^2$
$\leq \norm{x_t -x^*}^2 + \frac{2\lambda}{L} (f^* - f(x_t)) + \lambda^2$
$\rightarrow f(x_t) - f^* \leq \frac{L}{2 \lambda} (\norm{x_t - x^*}^2 - 
\norm{x_{t+1} - x^*}^2) + \frac{L \lambda}{2}$
$f(\bar{x}) - f^* \leq f(x_t) - f^*$
[recursive part?]
$\rightarrow T (f(\bar{x}) - f^*) \leq \frac{L}{2 \lambda} (\norm{x_0 -x^*}^2 - 
\underbrace{\norm{x_{T} - x^*}^2}_{\geq 0 \rightarrow \text{dropped}}) + T 
\frac{L\lambda}{2}$
$\leq \frac{L}{2\lambda} D^2 + T \frac{L\lambda}{2} \rightarrow f(x) - f^* \leq
\underbrace{\frac{L}{2\lambda T} D^2}_{=\frac{4\epsilon}{5}} +
\underbrace{\frac{L \lambda}{2}}_{=\frac{\epsilon}{5}} = \epsilon$.

\paragraph{Algorithm extends to subgradients}
The projection gradient method naturally extends to using subgradients $s$ such
that $||s||_2 \leq L$ instead of gradients.

\todo[inline]{algorithm as homework}

\subsection{Some further tool}

\paragraph{Theorem:}
Let $f_{1}, f_{2}, \dots, f_{m}$ be convex and twice continuous differentiable
functions. Let $D = \cap_{i = 1}^{m} dom (f_{i}) \neq \emptyset$. Let $f(x) =
\max \{f_1(x), \dots, f_m(x) \}$. (subfunctions are differentiable, but in some
points of $f(x)$ we need subgradients.)

\emph{For $x \in D, I(x) = \{i \in \{1, \dots, m\}, f(x) = f_i(x)\}$. Then,
$\partial f(x) = conv(\nabla f_i(x) \mid i \in I(x))$.}

\subparagraph{Proof:}
\begin{itemize}
\item "$conv(\nabla f_i(x) \mid i \in I(x)) \subseteq \partial f(x)$"
\subitem Take $x_0 \in D$. $\forall i \in I(x_0) \rightarrow f_i(x) - f_i(x_0)
\geq \nabla f_i^T(x-x_0) \forall x \in D$.\\
$f(x) - f(x_0) \underbrace{\geq}_{i \in I(x_0)} f_i(x) - f_i(x_0)$ ($f(x_0) =
f_i(x_0)$, but $f(x) \geq f_i(x)$)\\
$\geq \nabla f_i(x_0)^T(x-x_0) \forall x \in D$\\
$\rightarrow \nabla f_i(x_0) \in \partial f(x_0)$ $\forall i \in I(x_0)
\rightarrow conv(\nabla f_i(x_0) \mid i \in I(x_0)) \subseteq \partial f(x_0)$.
\item "$\partial f(x) \subseteq conv(\nabla f_i(x) \mid i \in I(x))$"
\subitem Let $x_0 \in D$. Conversely, suppose $\exists s \in \R^n$, $s \in
\partial f(x)$, $s \notin conv(\nabla f_i(x) \mid i \in I(x))$.\\
From the separate hyperplane theorem, $\exists d \in \R^n$, $p \in \R$ such
that $d^T h \leq p$, $\forall h \in conv(\nabla f_i(x) \mid i \in I(x))$,
\begin{equation}
\label{eq:*}
d^T s > p
\end{equation}
$f_i(x_0) < f(x_0) \forall i \notin I(x_0)$. $\exists \alpha > 0$ such that
$f(x_0 + \epsilon d) = f_k(x_0 + \epsilon d)$ for some $k \in I(x_0)$ and
$\forall 0 \leq \epsilon \leq \alpha$.
\begin{enumerate}
\item $f(x) - f(x_0) \geq s^T (x-x_0) \underbrace{=}_{x = x_0 + \epsilon d}
\epsilon d^T s$
\item $f(x) - f(x_0) = f_k(x_0 + \epsilon d) - f_k(x_0) = \nabla f_k(x_0)^T
(\epsilon d) + r(\epsilon)$, where $r(\epsilon)$ is a taylor approximation and
quadratic, i.e, $r(\epsilon) = \sigma(\epsilon^2) \rightarrow r(\epsilon) \leq
\beta \epsilon^2$ for some constant $\beta$.
\end{enumerate}
We know from~\ref{eq:*}: $\delta = d^T s - d^T \nabla f_k(x_0) > 0$.\\
From (1) and (2): $\epsilon d^T s \leq \epsilon d^T \nabla f_k(x_0) +
r(\epsilon) \rightarrow \epsilon \delta \leq r(\epsilon) \leq \beta
\epsilon^2$, which is a contradiction when $\epsilon \to 0$.
\end{itemize}

\end{document}
